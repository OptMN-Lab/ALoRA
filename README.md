Official implementation of *Rethinking Parameter Sharing for LLM Fine-Tuning with Multiple LoRAs*

For multi-task fine-tuning and federated fine-tuning, please refer to each folder for more details.
